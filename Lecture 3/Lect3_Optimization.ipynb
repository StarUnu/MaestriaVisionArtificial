{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMTyK2+b9UBgwoi9PX5MM3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###Strategy 1: A first very bad idea solution: Random search"],"metadata":{"id":"2F_PQ7nzXw7u"}},{"cell_type":"code","source":["import numpy as np\n"],"metadata":{"id":"FYZ3eR8rlXuM","executionInfo":{"status":"ok","timestamp":1753530862873,"user_tz":300,"elapsed":9,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JmKl89nJ7KO","executionInfo":{"status":"ok","timestamp":1753531076916,"user_tz":300,"elapsed":59178,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"223b50a9-f432-43e0-d6bf-0c8ccf6825b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["In attempt 0 the loss was 2.302625, best 2.302625\n","In attempt 1 the loss was 2.302580, best 2.302580\n","In attempt 2 the loss was 2.302569, best 2.302569\n","In attempt 3 the loss was 2.302565, best 2.302565\n","In attempt 4 the loss was 2.302617, best 2.302565\n","In attempt 5 the loss was 2.302635, best 2.302565\n","In attempt 6 the loss was 2.302587, best 2.302565\n","In attempt 7 the loss was 2.302618, best 2.302565\n","In attempt 8 the loss was 2.302594, best 2.302565\n","In attempt 9 the loss was 2.302609, best 2.302565\n","In attempt 10 the loss was 2.302614, best 2.302565\n","In attempt 11 the loss was 2.302603, best 2.302565\n","In attempt 12 the loss was 2.302584, best 2.302565\n","In attempt 13 the loss was 2.302606, best 2.302565\n","In attempt 14 the loss was 2.302603, best 2.302565\n","In attempt 15 the loss was 2.302604, best 2.302565\n","In attempt 16 the loss was 2.302610, best 2.302565\n","In attempt 17 the loss was 2.302577, best 2.302565\n","In attempt 18 the loss was 2.302620, best 2.302565\n","In attempt 19 the loss was 2.302587, best 2.302565\n","In attempt 20 the loss was 2.302604, best 2.302565\n","In attempt 21 the loss was 2.302590, best 2.302565\n","In attempt 22 the loss was 2.302601, best 2.302565\n","In attempt 23 the loss was 2.302634, best 2.302565\n","In attempt 24 the loss was 2.302609, best 2.302565\n","In attempt 25 the loss was 2.302598, best 2.302565\n","In attempt 26 the loss was 2.302622, best 2.302565\n","In attempt 27 the loss was 2.302584, best 2.302565\n","In attempt 28 the loss was 2.302579, best 2.302565\n","In attempt 29 the loss was 2.302634, best 2.302565\n","In attempt 30 the loss was 2.302556, best 2.302556\n","In attempt 31 the loss was 2.302595, best 2.302556\n","In attempt 32 the loss was 2.302554, best 2.302554\n","In attempt 33 the loss was 2.302593, best 2.302554\n","In attempt 34 the loss was 2.302576, best 2.302554\n","In attempt 35 the loss was 2.302606, best 2.302554\n","In attempt 36 the loss was 2.302622, best 2.302554\n","In attempt 37 the loss was 2.302623, best 2.302554\n","In attempt 38 the loss was 2.302607, best 2.302554\n","In attempt 39 the loss was 2.302599, best 2.302554\n","In attempt 40 the loss was 2.302613, best 2.302554\n","In attempt 41 the loss was 2.302618, best 2.302554\n","In attempt 42 the loss was 2.302597, best 2.302554\n","In attempt 43 the loss was 2.302643, best 2.302554\n","In attempt 44 the loss was 2.302614, best 2.302554\n","In attempt 45 the loss was 2.302607, best 2.302554\n","In attempt 46 the loss was 2.302591, best 2.302554\n","In attempt 47 the loss was 2.302560, best 2.302554\n","In attempt 48 the loss was 2.302570, best 2.302554\n","In attempt 49 the loss was 2.302566, best 2.302554\n","In attempt 50 the loss was 2.302610, best 2.302554\n","In attempt 51 the loss was 2.302580, best 2.302554\n","In attempt 52 the loss was 2.302584, best 2.302554\n","In attempt 53 the loss was 2.302611, best 2.302554\n","In attempt 54 the loss was 2.302588, best 2.302554\n","In attempt 55 the loss was 2.302608, best 2.302554\n","In attempt 56 the loss was 2.302623, best 2.302554\n","In attempt 57 the loss was 2.302588, best 2.302554\n","In attempt 58 the loss was 2.302580, best 2.302554\n","In attempt 59 the loss was 2.302578, best 2.302554\n","In attempt 60 the loss was 2.302589, best 2.302554\n","In attempt 61 the loss was 2.302539, best 2.302539\n","In attempt 62 the loss was 2.302595, best 2.302539\n","In attempt 63 the loss was 2.302599, best 2.302539\n","In attempt 64 the loss was 2.302578, best 2.302539\n","In attempt 65 the loss was 2.302618, best 2.302539\n","In attempt 66 the loss was 2.302599, best 2.302539\n","In attempt 67 the loss was 2.302606, best 2.302539\n","In attempt 68 the loss was 2.302592, best 2.302539\n","In attempt 69 the loss was 2.302582, best 2.302539\n","In attempt 70 the loss was 2.302619, best 2.302539\n","In attempt 71 the loss was 2.302599, best 2.302539\n","In attempt 72 the loss was 2.302582, best 2.302539\n","In attempt 73 the loss was 2.302588, best 2.302539\n","In attempt 74 the loss was 2.302559, best 2.302539\n","In attempt 75 the loss was 2.302639, best 2.302539\n","In attempt 76 the loss was 2.302572, best 2.302539\n","In attempt 77 the loss was 2.302596, best 2.302539\n","In attempt 78 the loss was 2.302590, best 2.302539\n","In attempt 79 the loss was 2.302608, best 2.302539\n","In attempt 80 the loss was 2.302593, best 2.302539\n","In attempt 81 the loss was 2.302641, best 2.302539\n","In attempt 82 the loss was 2.302568, best 2.302539\n","In attempt 83 the loss was 2.302629, best 2.302539\n","In attempt 84 the loss was 2.302581, best 2.302539\n","In attempt 85 the loss was 2.302603, best 2.302539\n","In attempt 86 the loss was 2.302565, best 2.302539\n","In attempt 87 the loss was 2.302604, best 2.302539\n","In attempt 88 the loss was 2.302594, best 2.302539\n","In attempt 89 the loss was 2.302583, best 2.302539\n","In attempt 90 the loss was 2.302553, best 2.302539\n","In attempt 91 the loss was 2.302596, best 2.302539\n","In attempt 92 the loss was 2.302621, best 2.302539\n","In attempt 93 the loss was 2.302605, best 2.302539\n","In attempt 94 the loss was 2.302568, best 2.302539\n","In attempt 95 the loss was 2.302589, best 2.302539\n","In attempt 96 the loss was 2.302611, best 2.302539\n","In attempt 97 the loss was 2.302613, best 2.302539\n","In attempt 98 the loss was 2.302601, best 2.302539\n","In attempt 99 the loss was 2.302599, best 2.302539\n"]}],"source":["# Función de pérdida de ejemplo (softmax + cross-entropy)\n","def L(X, y, W):\n","    scores = np.dot(W, X)\n","    scores -= np.max(scores, axis=0, keepdims=True)\n","    exp_scores = np.exp(scores)\n","    probs = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n","\n","    N = X.shape[1]\n","    correct_logprobs = -np.log(probs[y, range(N)])\n","    data_loss = np.sum(correct_logprobs) / N\n","    return data_loss\n","\n","# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n","# assume Y_train are the labels (e.g. 1D array of 50,000)\n","# assume the function L evaluates the loss function\n","X_train = np.random.randn(3073, 50000)\n","Y_train = np.random.randint(0, 10, 50000)\n","\n","bestloss = float(\"inf\") # Python assigns the highest possible float value\n","for num in range(100):\n","  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n","  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n","  if loss < bestloss: # keep track of the best solution\n","    bestloss = loss\n","    bestW = W\n","  print('In attempt %d the loss was %f, best %f' % (num, loss, bestloss))\n"]},{"cell_type":"code","source":["# Simula datos de entrenamiento y prueba\n","X_train = np.random.randn(3073, 50000)\n","Y_train = np.random.randint(0, 10, 50000)\n","\n","X_test = np.random.randn(3073, 10000)\n","Y_test = np.random.randint(0, 10, 10000)\n","\n","# Función de pérdida\n","def L(X, y, W):\n","    scores = np.dot(W, X)\n","    scores -= np.max(scores, axis=0, keepdims=True)\n","    exp_scores = np.exp(scores)\n","    probs = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n","    N = X.shape[1]\n","    correct_logprobs = -np.log(probs[y, range(N)])\n","    loss = np.sum(correct_logprobs) / N\n","    return loss\n","\n","# Búsqueda aleatoria de mejores pesos\n","bestloss = float(\"inf\")\n","for num in range(100):\n","    W = np.random.randn(10, 3073) * 0.0001\n","    loss = L(X_train, Y_train, W)\n","    if loss < bestloss:\n","        bestloss = loss\n","        bestW = W\n","    print(f\"In attempt {num} the loss was {loss:.6f}, best {bestloss:.6f}\")\n","\n","# Evaluación en test set\n","scores = bestW.dot(X_test)\n","Y_pred = np.argmax(scores, axis=0)\n","accuracy = np.mean(Y_pred == Y_test)\n","print(f\"Test accuracy: {accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iC0a4dYQGuU","executionInfo":{"status":"ok","timestamp":1753531145993,"user_tz":300,"elapsed":62083,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"3a96e04b-d4a3-49b8-c9b0-832ebadbbd50"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["In attempt 0 the loss was 2.302579, best 2.302579\n","In attempt 1 the loss was 2.302616, best 2.302579\n","In attempt 2 the loss was 2.302595, best 2.302579\n","In attempt 3 the loss was 2.302593, best 2.302579\n","In attempt 4 the loss was 2.302602, best 2.302579\n","In attempt 5 the loss was 2.302594, best 2.302579\n","In attempt 6 the loss was 2.302631, best 2.302579\n","In attempt 7 the loss was 2.302580, best 2.302579\n","In attempt 8 the loss was 2.302592, best 2.302579\n","In attempt 9 the loss was 2.302604, best 2.302579\n","In attempt 10 the loss was 2.302633, best 2.302579\n","In attempt 11 the loss was 2.302605, best 2.302579\n","In attempt 12 the loss was 2.302588, best 2.302579\n","In attempt 13 the loss was 2.302586, best 2.302579\n","In attempt 14 the loss was 2.302609, best 2.302579\n","In attempt 15 the loss was 2.302625, best 2.302579\n","In attempt 16 the loss was 2.302653, best 2.302579\n","In attempt 17 the loss was 2.302564, best 2.302564\n","In attempt 18 the loss was 2.302613, best 2.302564\n","In attempt 19 the loss was 2.302623, best 2.302564\n","In attempt 20 the loss was 2.302592, best 2.302564\n","In attempt 21 the loss was 2.302563, best 2.302563\n","In attempt 22 the loss was 2.302600, best 2.302563\n","In attempt 23 the loss was 2.302590, best 2.302563\n","In attempt 24 the loss was 2.302642, best 2.302563\n","In attempt 25 the loss was 2.302601, best 2.302563\n","In attempt 26 the loss was 2.302627, best 2.302563\n","In attempt 27 the loss was 2.302573, best 2.302563\n","In attempt 28 the loss was 2.302600, best 2.302563\n","In attempt 29 the loss was 2.302593, best 2.302563\n","In attempt 30 the loss was 2.302581, best 2.302563\n","In attempt 31 the loss was 2.302533, best 2.302533\n","In attempt 32 the loss was 2.302648, best 2.302533\n","In attempt 33 the loss was 2.302625, best 2.302533\n","In attempt 34 the loss was 2.302617, best 2.302533\n","In attempt 35 the loss was 2.302597, best 2.302533\n","In attempt 36 the loss was 2.302574, best 2.302533\n","In attempt 37 the loss was 2.302620, best 2.302533\n","In attempt 38 the loss was 2.302568, best 2.302533\n","In attempt 39 the loss was 2.302610, best 2.302533\n","In attempt 40 the loss was 2.302627, best 2.302533\n","In attempt 41 the loss was 2.302594, best 2.302533\n","In attempt 42 the loss was 2.302582, best 2.302533\n","In attempt 43 the loss was 2.302568, best 2.302533\n","In attempt 44 the loss was 2.302629, best 2.302533\n","In attempt 45 the loss was 2.302607, best 2.302533\n","In attempt 46 the loss was 2.302606, best 2.302533\n","In attempt 47 the loss was 2.302602, best 2.302533\n","In attempt 48 the loss was 2.302627, best 2.302533\n","In attempt 49 the loss was 2.302648, best 2.302533\n","In attempt 50 the loss was 2.302586, best 2.302533\n","In attempt 51 the loss was 2.302638, best 2.302533\n","In attempt 52 the loss was 2.302618, best 2.302533\n","In attempt 53 the loss was 2.302587, best 2.302533\n","In attempt 54 the loss was 2.302571, best 2.302533\n","In attempt 55 the loss was 2.302576, best 2.302533\n","In attempt 56 the loss was 2.302602, best 2.302533\n","In attempt 57 the loss was 2.302585, best 2.302533\n","In attempt 58 the loss was 2.302632, best 2.302533\n","In attempt 59 the loss was 2.302594, best 2.302533\n","In attempt 60 the loss was 2.302590, best 2.302533\n","In attempt 61 the loss was 2.302579, best 2.302533\n","In attempt 62 the loss was 2.302621, best 2.302533\n","In attempt 63 the loss was 2.302568, best 2.302533\n","In attempt 64 the loss was 2.302634, best 2.302533\n","In attempt 65 the loss was 2.302606, best 2.302533\n","In attempt 66 the loss was 2.302599, best 2.302533\n","In attempt 67 the loss was 2.302578, best 2.302533\n","In attempt 68 the loss was 2.302614, best 2.302533\n","In attempt 69 the loss was 2.302625, best 2.302533\n","In attempt 70 the loss was 2.302598, best 2.302533\n","In attempt 71 the loss was 2.302613, best 2.302533\n","In attempt 72 the loss was 2.302553, best 2.302533\n","In attempt 73 the loss was 2.302624, best 2.302533\n","In attempt 74 the loss was 2.302583, best 2.302533\n","In attempt 75 the loss was 2.302585, best 2.302533\n","In attempt 76 the loss was 2.302623, best 2.302533\n","In attempt 77 the loss was 2.302577, best 2.302533\n","In attempt 78 the loss was 2.302588, best 2.302533\n","In attempt 79 the loss was 2.302593, best 2.302533\n","In attempt 80 the loss was 2.302590, best 2.302533\n","In attempt 81 the loss was 2.302639, best 2.302533\n","In attempt 82 the loss was 2.302580, best 2.302533\n","In attempt 83 the loss was 2.302591, best 2.302533\n","In attempt 84 the loss was 2.302606, best 2.302533\n","In attempt 85 the loss was 2.302589, best 2.302533\n","In attempt 86 the loss was 2.302612, best 2.302533\n","In attempt 87 the loss was 2.302586, best 2.302533\n","In attempt 88 the loss was 2.302650, best 2.302533\n","In attempt 89 the loss was 2.302609, best 2.302533\n","In attempt 90 the loss was 2.302581, best 2.302533\n","In attempt 91 the loss was 2.302593, best 2.302533\n","In attempt 92 the loss was 2.302565, best 2.302533\n","In attempt 93 the loss was 2.302567, best 2.302533\n","In attempt 94 the loss was 2.302622, best 2.302533\n","In attempt 95 the loss was 2.302574, best 2.302533\n","In attempt 96 the loss was 2.302643, best 2.302533\n","In attempt 97 the loss was 2.302587, best 2.302533\n","In attempt 98 the loss was 2.302626, best 2.302533\n","In attempt 99 the loss was 2.302611, best 2.302533\n","Test accuracy: 0.1007\n"]}]},{"cell_type":"markdown","source":["### Strategy 2: Random Local Search"],"metadata":{"id":"IKaPplxXYBeI"}},{"cell_type":"code","source":["# Simulación de datos de entrenamiento (si no tienes datos reales)\n","X_train = np.random.randn(3073, 50000)        # (D, N)\n","Y_train = np.random.randint(0, 10, 50000)     # (N,)\n","\n","# Función de pérdida softmax + cross-entropy\n","def L(X, y, W):\n","    scores = np.dot(W, X)\n","    scores -= np.max(scores, axis=0, keepdims=True)\n","    exp_scores = np.exp(scores)\n","    probs = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n","    N = X.shape[1]\n","    correct_logprobs = -np.log(probs[y, range(N)])\n","    loss = np.sum(correct_logprobs) / N\n","    return loss\n","\n","# Estrategia #2: Búsqueda local aleatoria\n","W = np.random.randn(10, 3073) * 0.001   # pesos iniciales\n","bestloss = float(\"inf\")\n","\n","step_size = 0.0001\n","\n","for i in range(100):\n","    Wtry = W + np.random.randn(10, 3073) * step_size\n","    loss = L(X_train, Y_train, Wtry)\n","\n","    if loss < bestloss:\n","        W = Wtry\n","        bestloss = loss\n","\n","    print(f\"Iter {i}, Best loss: {bestloss:.6f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvqxpgTUU9Si","executionInfo":{"status":"ok","timestamp":1753531208251,"user_tz":300,"elapsed":59250,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"8b6439f2-4ba0-446c-8720-10c36866c265"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 0, Best loss: 2.303958\n","Iter 1, Best loss: 2.303958\n","Iter 2, Best loss: 2.303958\n","Iter 3, Best loss: 2.303935\n","Iter 4, Best loss: 2.303935\n","Iter 5, Best loss: 2.303935\n","Iter 6, Best loss: 2.303935\n","Iter 7, Best loss: 2.303928\n","Iter 8, Best loss: 2.303920\n","Iter 9, Best loss: 2.303919\n","Iter 10, Best loss: 2.303916\n","Iter 11, Best loss: 2.303916\n","Iter 12, Best loss: 2.303895\n","Iter 13, Best loss: 2.303881\n","Iter 14, Best loss: 2.303857\n","Iter 15, Best loss: 2.303853\n","Iter 16, Best loss: 2.303848\n","Iter 17, Best loss: 2.303848\n","Iter 18, Best loss: 2.303848\n","Iter 19, Best loss: 2.303835\n","Iter 20, Best loss: 2.303835\n","Iter 21, Best loss: 2.303835\n","Iter 22, Best loss: 2.303808\n","Iter 23, Best loss: 2.303808\n","Iter 24, Best loss: 2.303808\n","Iter 25, Best loss: 2.303808\n","Iter 26, Best loss: 2.303806\n","Iter 27, Best loss: 2.303806\n","Iter 28, Best loss: 2.303785\n","Iter 29, Best loss: 2.303785\n","Iter 30, Best loss: 2.303767\n","Iter 31, Best loss: 2.303767\n","Iter 32, Best loss: 2.303761\n","Iter 33, Best loss: 2.303761\n","Iter 34, Best loss: 2.303761\n","Iter 35, Best loss: 2.303761\n","Iter 36, Best loss: 2.303752\n","Iter 37, Best loss: 2.303752\n","Iter 38, Best loss: 2.303752\n","Iter 39, Best loss: 2.303752\n","Iter 40, Best loss: 2.303728\n","Iter 41, Best loss: 2.303728\n","Iter 42, Best loss: 2.303728\n","Iter 43, Best loss: 2.303728\n","Iter 44, Best loss: 2.303720\n","Iter 45, Best loss: 2.303718\n","Iter 46, Best loss: 2.303718\n","Iter 47, Best loss: 2.303718\n","Iter 48, Best loss: 2.303718\n","Iter 49, Best loss: 2.303682\n","Iter 50, Best loss: 2.303682\n","Iter 51, Best loss: 2.303638\n","Iter 52, Best loss: 2.303638\n","Iter 53, Best loss: 2.303622\n","Iter 54, Best loss: 2.303622\n","Iter 55, Best loss: 2.303622\n","Iter 56, Best loss: 2.303622\n","Iter 57, Best loss: 2.303622\n","Iter 58, Best loss: 2.303622\n","Iter 59, Best loss: 2.303622\n","Iter 60, Best loss: 2.303622\n","Iter 61, Best loss: 2.303622\n","Iter 62, Best loss: 2.303622\n","Iter 63, Best loss: 2.303591\n","Iter 64, Best loss: 2.303591\n","Iter 65, Best loss: 2.303591\n","Iter 66, Best loss: 2.303591\n","Iter 67, Best loss: 2.303585\n","Iter 68, Best loss: 2.303585\n","Iter 69, Best loss: 2.303585\n","Iter 70, Best loss: 2.303576\n","Iter 71, Best loss: 2.303576\n","Iter 72, Best loss: 2.303576\n","Iter 73, Best loss: 2.303576\n","Iter 74, Best loss: 2.303576\n","Iter 75, Best loss: 2.303576\n","Iter 76, Best loss: 2.303551\n","Iter 77, Best loss: 2.303551\n","Iter 78, Best loss: 2.303551\n","Iter 79, Best loss: 2.303551\n","Iter 80, Best loss: 2.303548\n","Iter 81, Best loss: 2.303548\n","Iter 82, Best loss: 2.303548\n","Iter 83, Best loss: 2.303548\n","Iter 84, Best loss: 2.303548\n","Iter 85, Best loss: 2.303548\n","Iter 86, Best loss: 2.303548\n","Iter 87, Best loss: 2.303530\n","Iter 88, Best loss: 2.303530\n","Iter 89, Best loss: 2.303530\n","Iter 90, Best loss: 2.303530\n","Iter 91, Best loss: 2.303530\n","Iter 92, Best loss: 2.303530\n","Iter 93, Best loss: 2.303530\n","Iter 94, Best loss: 2.303525\n","Iter 95, Best loss: 2.303489\n","Iter 96, Best loss: 2.303482\n","Iter 97, Best loss: 2.303482\n","Iter 98, Best loss: 2.303461\n","Iter 99, Best loss: 2.303461\n"]}]},{"cell_type":"markdown","source":["###Strategy 3: Following the Gradient"],"metadata":{"id":"o4MpT5VJYRNo"}},{"cell_type":"code","source":["def eval_numerical_gradient(f, x):\n","  \"\"\"\n","  a naive implementation of numerical gradient of f at x\n","  - f should be a function that takes a single argument\n","  - x is the point (numpy array) to evaluate the gradient at\n","  \"\"\"\n","\n","  fx = f(x) # evaluate function value at original point\n","  grad = np.zeros(x.shape)\n","  h = 0.00001\n","\n","  # iterate over all indexes in x\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","\n","    # evaluate function at x+h\n","    ix = it.multi_index\n","    old_value = x[ix]\n","    x[ix] = old_value + h # increment by h\n","    fxh = f(x) # evalute f(x + h)\n","    x[ix] = old_value # restore to previous value (very important!)\n","\n","    # compute the partial derivative\n","    grad[ix] = (fxh - fx) / h # the slope\n","    it.iternext() # step to next dimension\n","\n","  return grad"],"metadata":{"id":"hSxDgd8CYUYS","executionInfo":{"status":"ok","timestamp":1753531212576,"user_tz":300,"elapsed":26,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# to use the generic code above we want a function that takes a single argument\n","# (the weights in our case) so we close over X_train and Y_train\n","def CIFAR10_loss_fun(W, batch_size=1000):\n","  indices = np.random.choice(X_train.shape[1], 1000, replace=False)\n","  X_batch = X_train[:, indices] #mini-batch\n","  Y_batch = Y_train[indices]  #mini-batch\n","  return L(X_train, Y_train, W)\n","\n","W = np.random.rand(10, 3073) * 0.001 # random weight vector\n","#df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient\n","batch_size = 1000\n","fixed_indices = np.random.choice(X_train.shape[1], batch_size, replace=False)\n","X_batch_fixed = X_train[:, fixed_indices]\n","Y_batch_fixed = Y_train[fixed_indices]\n","\n","def CIFAR10_loss_fun_fixed(W):\n","    return L(X_batch_fixed, Y_batch_fixed, W)\n","\n","# Ahora sí, calcula el gradiente numérico con el mini-batch fijo\n","df = eval_numerical_gradient(CIFAR10_loss_fun_fixed, W)"],"metadata":{"id":"mPbdvaaeYeGI","executionInfo":{"status":"ok","timestamp":1753531522830,"user_tz":300,"elapsed":305950,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["loss_original = CIFAR10_loss_fun(W, batch_size=1000) # the original loss\n","print ('original loss: %f' % (loss_original, ))\n","\n","# lets see the effect of multiple step sizes\n","for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n","  step_size = 10 ** step_size_log\n","  W_new = W - step_size * df # new position in the weight space\n","  loss_new = CIFAR10_loss_fun(W_new)\n","  print ('for step size %f new loss: %f' % (step_size, loss_new))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0qs0-_wZRe4","executionInfo":{"status":"ok","timestamp":1753531554420,"user_tz":300,"elapsed":6148,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"f678e83a-f967-4186-f7bf-e06743115d54"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["original loss: 2.302689\n","for step size 0.000000 new loss: 2.302689\n","for step size 0.000000 new loss: 2.302689\n","for step size 0.000000 new loss: 2.302689\n","for step size 0.000000 new loss: 2.302689\n","for step size 0.000001 new loss: 2.302688\n","for step size 0.000010 new loss: 2.302688\n","for step size 0.000100 new loss: 2.302683\n","for step size 0.001000 new loss: 2.302632\n","for step size 0.010000 new loss: 2.302137\n","for step size 0.100000 new loss: 2.298517\n"]}]},{"cell_type":"code","source":["# Vanilla Minibatch Gradient Descent\n","\n","#while True:\n","  #weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n","  #weights += - step_size * weights_grad # perform parameter update\n","\n","import numpy as np\n","\n","# Supón que ya tienes tus datos en estas variables:\n","# X_train: (3073, 50000)\n","# Y_train: (50000,)\n","# X_test, Y_test igual forma\n","\n","def softmax_loss(X, y, W):\n","    N = X.shape[1]\n","    scores = W @ X\n","    scores -= np.max(scores, axis=0, keepdims=True)\n","    probs = np.exp(scores) / np.sum(np.exp(scores), axis=0, keepdims=True)\n","\n","    loss = -np.sum(np.log(probs[y, range(N)])) / N\n","\n","    dscores = probs.copy()\n","    dscores[y, range(N)] -= 1\n","    dscores /= N\n","\n","    dW = dscores @ X.T\n","    return loss, dW\n","\n","# Inicializar pesos\n","W = 0.001 * np.random.randn(10, 3073)\n","step_size = 1e-7\n","num_iters = 1000\n","batch_size = 1000\n","\n","for i in range(num_iters):\n","    # Seleccionar mini-batch aleatorio\n","    indices = np.random.choice(X_train.shape[1], batch_size, replace=False)\n","    X_batch = X_train[:, indices]\n","    Y_batch = Y_train[indices]\n","\n","    # Calcular pérdida y gradiente\n","    loss, grad = softmax_loss(X_batch, Y_batch, W)\n","\n","    # Actualizar pesos\n","    W -= step_size * grad\n","\n","    # Mostrar progreso\n","    if i % 100 == 0:\n","        print(f\"Iteración {i}: loss = {loss:.4f}\")\n","\n","# Evaluar en test set\n","scores = W @ X_test\n","Y_pred = np.argmax(scores, axis=0)\n","acc = np.mean(Y_pred == Y_test)\n","print(f\"Precisión en test: {acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCdkQ0GmZkZJ","executionInfo":{"status":"ok","timestamp":1753532309166,"user_tz":300,"elapsed":77724,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"45829ea7-1441-4096-d2fe-9d60adaf021f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteración 0: loss = 2.3019\n","Iteración 100: loss = 2.3058\n","Iteración 200: loss = 2.3038\n","Iteración 300: loss = 2.3086\n","Iteración 400: loss = 2.3011\n","Iteración 500: loss = 2.3051\n","Iteración 600: loss = 2.3026\n","Iteración 700: loss = 2.3054\n","Iteración 800: loss = 2.3041\n","Iteración 900: loss = 2.3020\n","Precisión en test: 0.1009\n"]}]},{"cell_type":"code","source":["# Vanilla Minibatch Gradient Descent\n","\n","#while True:\n","  #data_batch = sample_training_data(data, 256) # sample 256 examples\n","  #weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n","  #weights += - step_size * weights_grad # perform parameter update\n","\n","import numpy as np\n","\n","# Supón que tienes X_train (3073, N), Y_train (N,)\n","# También tienes X_test, Y_test para evaluación\n","\n","def softmax_loss(X, y, W):\n","    N = X.shape[1]\n","    scores = W @ X\n","    scores -= np.max(scores, axis=0, keepdims=True)\n","\n","    probs = np.exp(scores) / np.sum(np.exp(scores), axis=0, keepdims=True)\n","    loss = -np.sum(np.log(probs[y, range(N)])) / N\n","\n","    dscores = probs\n","    dscores[y, range(N)] -= 1\n","    dscores /= N\n","\n","    dW = dscores @ X.T\n","    return loss, dW\n","\n","# Inicialización\n","W = 0.001 * np.random.randn(10, 3073)\n","step_size = 1e-7\n","num_iters = 1000\n","batch_size = 256\n","\n","for i in range(num_iters):\n","    # Mini-batch aleatorio\n","    indices = np.random.choice(X_train.shape[1], batch_size, replace=False)\n","    X_batch = X_train[:, indices]\n","    Y_batch = Y_train[indices]\n","\n","    # Calcular pérdida y gradiente\n","    loss, grad = softmax_loss(X_batch, Y_batch, W)\n","\n","    # Actualización de pesos\n","    W -= step_size * grad\n","\n","    if i % 100 == 0:\n","        print(f\"Iter {i}: loss = {loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feKYhebnq5NT","executionInfo":{"status":"ok","timestamp":1753532403818,"user_tz":300,"elapsed":22346,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"684c75bc-5fdd-4e4c-85fb-c12720933530"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 0: loss = 2.3061\n","Iter 100: loss = 2.3009\n","Iter 200: loss = 2.3063\n","Iter 300: loss = 2.3005\n","Iter 400: loss = 2.2941\n","Iter 500: loss = 2.3074\n","Iter 600: loss = 2.3063\n","Iter 700: loss = 2.2973\n","Iter 800: loss = 2.3012\n","Iter 900: loss = 2.3063\n"]}]},{"cell_type":"code","source":["scores_test = W @ X_test\n","Y_pred = np.argmax(scores_test, axis=0)\n","acc = np.mean(Y_pred == Y_test)\n","print(f\"Test accuracy: {acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJnWf7xqrNwK","executionInfo":{"status":"ok","timestamp":1753532421497,"user_tz":300,"elapsed":14,"user":{"displayName":"SOLEDAD MARGARITA PINEDA CUADROS","userId":"11356833002729101284"}},"outputId":"5602cff2-afa4-443f-85d4-171714185082"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.0993\n"]}]}]}