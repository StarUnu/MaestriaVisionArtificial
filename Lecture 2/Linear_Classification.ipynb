{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTTjqT95Y96R"
      },
      "outputs": [],
      "source": [
        "def L_i(x, y, W):\n",
        "  \"\"\"\n",
        "  L_i: versión sin vectorizar (para un solo ejemplo).\n",
        "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
        "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
        "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
        "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
        "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
        "  \"\"\"\n",
        "  delta = 1.0 # see notes about delta later in this section\n",
        "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
        "  correct_class_score = scores[y]\n",
        "  D = W.shape[0] # number of classes, e.g. 10\n",
        "  loss_i = 0.0\n",
        "  for j in range(D): # iterate over all wrong classes\n",
        "    if j == y:\n",
        "      # skip for the true class to only loop over incorrect classes\n",
        "      continue\n",
        "    # accumulate loss for the i-th example\n",
        "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
        "  return loss_i\n",
        "\n",
        "def L_i_vectorized(x, y, W):\n",
        "  \"\"\"\n",
        "  L_i_vectorized: versión medio vectorizada (aún solo para un ejemplo).\n",
        "  A faster half-vectorized implementation. half-vectorized\n",
        "  refers to the fact that for a single example the implementation contains\n",
        "  no for loops, but there is still one loop over the examples (outside this function)\n",
        "  \"\"\"\n",
        "  delta = 1.0\n",
        "  scores = W.dot(x)\n",
        "  # compute the margins for all classes in one vector operation\n",
        "  margins = np.maximum(0, scores - scores[y] + delta)\n",
        "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
        "  # to ignore the y-th position and only consider margin on max wrong class\n",
        "  margins[y] = 0\n",
        "  loss_i = np.sum(margins)\n",
        "  return loss_i\n",
        "\n",
        "def L(X, y, W):\n",
        "  \"\"\"\n",
        "  Fully-vectorized SVM loss.\n",
        "  - X: matriz de datos (N x D), cada fila es un ejemplo\n",
        "  - y: vector de etiquetas correctas (N,)\n",
        "  - W: matriz de pesos (C x D)\n",
        "  Retorna:\n",
        "  - pérdida escalar (float)\n",
        "  \"\"\"\n",
        "\n",
        "  delta = 1.0\n",
        "  scores = X.dot(W.T)  # (N x C): cada fila contiene scores por clase\n",
        "  N = X.shape[0]\n",
        "\n",
        "  # Obtener score correcto para cada fila (N x 1)\n",
        "  correct_class_scores = scores[np.arange(N), y].reshape(-1, 1)\n",
        "\n",
        "  # Calcular márgenes\n",
        "  margins = np.maximum(0, scores - correct_class_scores + delta)\n",
        "  margins[np.arange(N), y] = 0  # Ignorar la clase correcta\n",
        "\n",
        "  # Pérdida total promedio\n",
        "  loss = np.sum(margins) / N\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simular pesos (3 clases, 5 características)\n",
        "W = np.array([\n",
        "  [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "  [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "  [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "])\n",
        "\n",
        "# Simular 4 ejemplos de entrada\n",
        "X = np.array([\n",
        "  [1, 0, 0, 0, 1],\n",
        "  [0, 1, 0, 1, 0],\n",
        "  [1, 1, 1, 0, 0],\n",
        "  [0, 0, 0, 1, 1]\n",
        "])\n",
        "\n",
        "# Etiquetas correctas\n",
        "y = np.array([0, 1, 2, 1])\n",
        "\n",
        "# Calcular pérdida\n",
        "print(\"Loss:\", L(X, y, W))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kVGJrgmgbNO",
        "outputId": "8cb390d4-354f-4b64-c349-153710a75b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss(X, y, W, delta=1.0):\n",
        "    N = X.shape[0]\n",
        "    scores = X.dot(W.T)  # (N x C)\n",
        "    correct_scores = scores[np.arange(N), y].reshape(-1, 1)\n",
        "    margins = np.maximum(0, scores - correct_scores + delta)\n",
        "    margins[np.arange(N), y] = 0\n",
        "    loss = np.sum(margins) / N\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "VAE2RzSRho7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_loss(X, y, W):\n",
        "    N = X.shape[0]\n",
        "    scores = X.dot(W.T)\n",
        "\n",
        "    # Evitar explosión exponencial (trick numérico)\n",
        "    scores -= np.max(scores, axis=1, keepdims=True)\n",
        "\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    correct_logprobs = -np.log(probs[np.arange(N), y])\n",
        "    loss = np.sum(correct_logprobs) / N\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "NCDrzN6aiBWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = svm_loss(X, y, W)\n",
        "softmax = softmax_loss(X, y, W)\n",
        "\n",
        "print(f\"Pérdida SVM:     {svm:.4f}\")\n",
        "print(f\"Pérdida Softmax: {softmax:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgrBpYOEiMb2",
        "outputId": "8b1f24ea-d081-4908-a80d-3917adaded1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pérdida SVM:     2.4500\n",
            "Pérdida Softmax: 1.2717\n"
          ]
        }
      ]
    }
  ]
}