{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgPjSzzsCKCU",
        "outputId": "9bfdfddb-267f-44fa-a35a-2b97a67dd2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Cargar CIFAR-10 desde torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Descargar y cargar los datos\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Convertir a numpy\n",
        "X_train = np.array([np.array(img[0]).transpose(1, 2, 0) for img in trainset])\n",
        "y_train = np.array(trainset.targets)\n",
        "\n",
        "X_test = np.array([np.array(img[0]).transpose(1, 2, 0) for img in testset])\n",
        "y_test = np.array(testset.targets)\n",
        "\n",
        "# Submuestreo para rapidez\n",
        "X_train = X_train[:5000]\n",
        "y_train = y_train[:5000]\n",
        "X_test  = X_test[:500]\n",
        "y_test  = y_test[:500]\n",
        "\n",
        "# Aplanar imágenes\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test  = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Normalizar datos\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "X_train -= mean_image\n",
        "X_test  -= mean_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss_naive(W, X, y, reg):\n",
        "    dW = np.zeros_like(W)\n",
        "    num_classes = W.shape[1]\n",
        "    num_train = X.shape[0]\n",
        "    loss = 0.0\n",
        "\n",
        "    for i in range(num_train):\n",
        "        scores = X[i].dot(W)\n",
        "        correct_class_score = scores[y[i]]\n",
        "        for j in range(num_classes):\n",
        "            if j == y[i]:\n",
        "                continue\n",
        "            margin = scores[j] - correct_class_score + 1  # delta = 1\n",
        "            if margin > 0:\n",
        "                loss += margin\n",
        "                dW[:, j] += X[i]\n",
        "                dW[:, y[i]] -= X[i]\n",
        "\n",
        "    loss /= num_train\n",
        "    dW   /= num_train\n",
        "\n",
        "    # Regularización\n",
        "    loss += 0.5 * reg * np.sum(W * W)\n",
        "    dW   += reg * W\n",
        "    return loss, dW\n"
      ],
      "metadata": {
        "id": "heC9yAQECRX5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar pesos aleatoriamente\n",
        "np.random.seed(0)\n",
        "W = 0.001 * np.random.randn(X_train.shape[1], 10)\n",
        "\n",
        "# Hiperparámetros\n",
        "learning_rate = 1e-7\n",
        "reg = 2.5e4\n",
        "num_iters = 1500\n",
        "\n",
        "# Entrenamiento con descenso de gradiente\n",
        "for it in range(num_iters):\n",
        "    loss, grad = svm_loss_naive(W, X_train, y_train, reg)\n",
        "    W -= learning_rate * grad\n",
        "\n",
        "    if it % 100 == 0:\n",
        "        print(f'Iter {it}/{num_iters}: Loss {loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHlauo3WCVdC",
        "outputId": "0f5ff49a-5cac-4b64-d062-97ee98c11311"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0/1500: Loss 388.3619\n",
            "Iter 100/1500: Loss 238.9495\n",
            "Iter 200/1500: Loss 148.3829\n",
            "Iter 300/1500: Loss 93.4860\n",
            "Iter 400/1500: Loss 60.2101\n",
            "Iter 500/1500: Loss 40.0399\n",
            "Iter 600/1500: Loss 27.8137\n",
            "Iter 700/1500: Loss 20.4028\n",
            "Iter 800/1500: Loss 15.9106\n",
            "Iter 900/1500: Loss 13.1877\n",
            "Iter 1000/1500: Loss 11.5372\n",
            "Iter 1100/1500: Loss 10.5368\n",
            "Iter 1200/1500: Loss 9.9303\n",
            "Iter 1300/1500: Loss 9.5628\n",
            "Iter 1400/1500: Loss 9.3399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones\n",
        "y_pred = np.argmax(X_test.dot(W), axis=1)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f'Accuracy - precisionn en test: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoFc52jMIe2g",
        "outputId": "5256f4d9-c19b-40d1-891b-3dcd1575dc34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy en test: 0.2220\n"
          ]
        }
      ]
    }
  ]
}